{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\nimport time"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["#Avvia la Spark Session mappando con delle confguration input e output da  verso MongoDB\n#Viene specificata l'opzione secondary preferred per indicare al driver di leggere dal secondario di MongoDB, se possibile.\n\nspark = SparkSession \\\n    .builder \\\n    .appName(\"myApp\") \\\n    .config(\"spark.mongodb.input.readPreference.name\", \"secondaryPreferred\") \\\n    .getOrCreate()"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["#Lettura del dataset da MongoDB\nsyn_dataset_mongo = spark.read.format(\"mongo\").option(\"spark.mongodb.input.uri\", \"mongodb+srv://ml-user:ml-user@cluster0-hkzmd.mongodb.net/test?retryWrites=true&w=majority\").option(\"database\", \"test\").option(\"collection\", \"test.syn-dos_double\").load()\n\n#Drop della colonna _id che non Ã¨ necessaria\nsyn_dataset_mongo = syn_dataset_mongo.drop(\"_id\")"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["#Explorazione del dataset\nsyn_dataset_mongo.show()"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["from pyspark.ml.feature import RFormula\n\n#Preparazione delle features per il Training\n#L'operazione crea il vettore delle features e delle label + indicizza il campo di testo presente in label\nassemblerFormula = RFormula(formula=\"Class ~ .\")\ntrainingTF = assemblerFormula.fit(syn_dataset_mongo)\nsyn_dataset_prepared = trainingTF.transform(syn_dataset_mongo).select(\"features\", \"label\")\nsyn_dataset_prepared.printSchema()\n\n#Creazione di training set e test set\ntraining, test = syn_dataset_prepared.randomSplit([70.0,30.0],seed=13)\nprint(\"Training Tot instances: \", training.count())\nprint(\"Test Tot instances: \", test.count())\n\n#TRAINING utilizzando il Ramdom Forest Classifier \n#https://spark.apache.org/docs/2.2.0/ml-classification-regression.html#random-forest-classifier\nalg_label=\"RANDOM FOREST CLASSIFIER\"\nfrom pyspark.ml.classification import RandomForestClassifier\nstart_time = time.time()\nrf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\",numTrees=30, maxDepth=10)\nrf_fitted = rf.fit(training)\nprint(\"%s Training time: %5.2f seconds ---\" % (alg_label,(time.time() - start_time)))\nstart_time = time.time()"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["#Stampa del modello\nprint(rf_fitted.toDebugString)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["#Evaluate model Accuracy\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\nrf_predictions_and_labels = rf_fitted.transform(test)\nevaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",\n                                              metricName=\"accuracy\")\nprint (\"Accuracy: %3.2f%%\" % (evaluator.evaluate(rf_predictions_and_labels)*100))\n"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["#Creazione di una Temp View\nrf_predictions_and_labels.createTempView(\"syn_dos_prediction\")"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["#Explorazione utilizzando SQL\nspark.sql(\"select * from syn_dos_prediction where prediction=1.0 limit 10\").show()"],"metadata":{},"outputs":[],"execution_count":9}],"metadata":{"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.5.2+","nbconvert_exporter":"python","file_extension":".py"},"name":"20190716-Basic_1","notebookId":1444136878635964,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"anaconda-cloud":{}},"nbformat":4,"nbformat_minor":0}
