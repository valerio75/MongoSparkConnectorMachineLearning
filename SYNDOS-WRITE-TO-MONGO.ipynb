{"cells":[{"cell_type":"code","source":["MONGO_URI=\"<Insert here MongoDB Atlas connection URI>\"\n\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\n\n#Avvia la Spark Session mappando con delle confguration input e output da  verso MongoDB\nspark = SparkSession \\\n    .builder \\\n    .appName(\"myApp\") \\\n    .getOrCreate()"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["#Lettura del file csv\nsyn_dataset=spark.read.option(\"header\", \"false\").option(\"inferschema\",\"true\").csv(\"/FileStore/tables/syn_dos_11000.csv\")\n\n#Conversione esplicita delle colonne in Double\nsyn_dataset_double = syn_dataset.select(*(col(c).cast(\"Double\").alias(c) for c in syn_dataset.columns[:-1]),col(syn_dataset.columns[-1])).withColumnRenamed(\"_c115\",\"Class\")"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["#Scrittura su MongoDB\nsyn_dataset_double.write.format(\"mongo\").option(\"spark.mongodb.output.uri\", MONGO_URI).option(\"database\", \"test\").option(\"collection\", \"test.syn-dos_double\").save()"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":4}],"metadata":{"name":"20191111-SYNDOS-WRITE-TO-MONGO","notebookId":1602733774232485},"nbformat":4,"nbformat_minor":0}
